{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyavaishnavichepuri/iiit-summer-internship-25/blob/main/task2journal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2650ef19",
      "metadata": {
        "id": "2650ef19"
      },
      "source": [
        "# iiit srishti task2\n",
        "\n",
        "### task word-to-word:\n",
        "\n",
        "(a) you are required to write code to perform segmentation on a \"list of images\" from your local folder in your computer. i think the relevant part of code snippet can be searched, retrieved (from ultralytics documentation pages) and inserted in your code. once you have done this, i.e., for achieving segmented output for multiple images, the code needs to be extended for a piece of video.\n",
        "\n",
        "(b) you should retrieve a video snippet from youtube or other social media forum. (please see that video selected doesn't hurt the feelings/sentiments of others in this group).\n",
        "\n",
        "(c) the video should be split into images at some regular intervals (try using ffmpeg for this, other tools are also available).\n",
        "\n",
        "(d) then after segmentation of all images extracted from this video, all the images should be joined and converted to a new video (use ffmpeg if other simpler tools aren't available).\n",
        "\n",
        "let the second part of fun begin !! one whole week to complete. take your time, go through tutorials/documentations on web, and solve. cheers...\n",
        "\n",
        "---\n",
        "\n",
        "### my exploration and understanding of each part\n",
        "\n",
        "### (a) segmentation on a list of local images\n",
        "\n",
        "i first had to figure out how yolov8 handles segmentation, not just detection, especially in video. segmentation means identifying which pixels belong to which object, which is much more detailed than simply drawing boxes.\n",
        "\n",
        "i wrote a script that loads all images from a local folder, applies segmentation, and saves the results in a separate folder. the key method was `model.predict()` and passing a folder path instead of a single image. i had to make sure all images were in the same format and placed in a folder before running the script.\n",
        "\n",
        "### (b) retrieving a video\n",
        "\n",
        "i tried using `yt-dlp.exe` to download a youtube video, but the command `./yt-dlp.exe` didn’t work in git bash. later i realized that syntax probably does not work on git cmd. since i was already juggling between environments and didn’t want to waste time, i just used a browser-based youtube downloader, y2mate to save an mp4 and placed it in the `task2` folder.\n",
        "\n",
        "### (c) splitting the video using ffmpeg\n",
        "\n",
        "what exactly is `ffmpeg?` ffmpeg is a powerful open-source tool used to process videos and audio. it can convert formats, extract frames, stitch images into a video, and much more. it works via command line, and once installed and added to the system path, i could run it from anywhere.\n",
        "\n",
        "to split the video into frames, i used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fbcca3",
      "metadata": {
        "id": "85fbcca3"
      },
      "outputs": [],
      "source": [
        "ffmpeg -i downloaded.mp4 -vf fps=1 frames/frame_%04d.jpg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae42cc8",
      "metadata": {
        "id": "eae42cc8"
      },
      "source": [
        "this extracts one frame per second and names them sequentially. i created a folder called `frames` beforehand to keep things neat.\n",
        "\n",
        "### (d) segmenting the frames and converting back to video\n",
        "\n",
        "after extracting the frames, i reused my segmentation script but pointed it to the `frames/` folder. yolov8 segmented each frame and saved the output into `runs/segment/predict/`. to create a video back from the segmented images, i used another ffmpeg command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e13769",
      "metadata": {
        "id": "e2e13769"
      },
      "outputs": [],
      "source": [
        "ffmpeg -framerate 1 -i runs/segment/predict/image%04d.jpg -c:v libx264 -pix_fmt yuv420p output_video.mp4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048268f1",
      "metadata": {
        "id": "048268f1"
      },
      "source": [
        "this stitched the segmented images into a new video.\n",
        "\n",
        "---\n",
        "\n",
        "### implementation part:\n",
        "\n",
        "## the photos:\n",
        "\n",
        "i downloaded some images from pinterest\n",
        "\n",
        "### segmentation on local images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d8c719",
      "metadata": {
        "id": "88d8c719"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "model = YOLO(\"yolov8x-seg.pt\")\n",
        "input_folder = \"local_images\"\n",
        "output_folder = \"segmented_images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "count = 1\n",
        "for img in sorted(os.listdir(input_folder)):\n",
        "    if img.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        path = os.path.join(input_folder, img)\n",
        "        results = model.predict(source=path, save=False, stream=True)\n",
        "        for r in results:\n",
        "            seg_img = r.plot()\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"{count:04d}.jpg\"), seg_img)\n",
        "            print(f\"Segmented {img}\")\n",
        "            count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59270765",
      "metadata": {
        "id": "59270765"
      },
      "source": [
        "### extract frames from video using ffmpeg\n",
        "\n",
        "first, i downloaded ffmpeg from the official site, and then, i added it to path. i then downloaded a yt video- the trailer of “air” from amazon mgm studios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8982297",
      "metadata": {
        "id": "c8982297"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir frames\n",
        "ffmpeg -i downloaded.mp4 -vf fps=1 frames/frame_%04d.jpg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d552a1b6",
      "metadata": {
        "id": "d552a1b6"
      },
      "source": [
        "### segmentation on extracted video frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee57e3f",
      "metadata": {
        "id": "eee57e3f"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "model = YOLO(\"yolov8x-seg.pt\")\n",
        "input_folder = \"frames\"\n",
        "output_folder = \"segmented_frames\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "count = 1\n",
        "for img in sorted(os.listdir(input_folder)):\n",
        "    if img.endswith(\".jpg\"):\n",
        "        path = os.path.join(input_folder, img)\n",
        "        results = model.predict(source=path, save=False, stream=True)\n",
        "        for r in results:\n",
        "            seg_img = r.plot()\n",
        "            out_path = os.path.join(output_folder, f\"{count:04d}.jpg\")\n",
        "            cv2.imwrite(out_path, seg_img)\n",
        "            print(f\"Processed {img} → {out_path}\")\n",
        "            count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009bc485",
      "metadata": {
        "id": "009bc485"
      },
      "source": [
        "but then i realised this was quite choppy- for it only takes one frame per second. i decided to do something further and keep in mind that i have to take care of this while i am exploring further.\n",
        "\n",
        "---\n",
        "\n",
        "### folder structure i used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72313783",
      "metadata": {
        "id": "72313783"
      },
      "outputs": [],
      "source": [
        "C:\\Users\\madha\\iiitsrishti\\task2\\\n",
        "│\n",
        "├── downloaded.mp4\n",
        "├── yt-dlp.exe\n",
        "├── frames\\\n",
        "│   ├── frame_0001.jpg\n",
        "│   └── ...\n",
        "├── images\\\n",
        "├── runs\\\n",
        "│   └── segment\\\n",
        "│       └── predict\\\n",
        "│           ├── image0001.jpg\n",
        "│           └── ...\n",
        "├── task2_image_segmentation.py\n",
        "└── task2_video_segmentation.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1f5475",
      "metadata": {
        "id": "8d1f5475"
      },
      "source": [
        "---\n",
        "\n",
        "# further work- static video other-worldly object detection\n",
        "\n",
        "\"well the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait.....!\n",
        "\n",
        "i have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. but computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. we owe the world that much.\"\n",
        "\n",
        "according to one of the older papers i was going through on yolov3: an incremental improvement, the creator wanted us to find that.\n",
        "\n",
        "sooo, as a responsible student, i’ve decided to explore just a little bit into this. although i did not want to count zebras, i wanted to do something more thrilling — so i came up with this idea where i thought, if we can detect burglars at night time and raise an automatic alarm, it could be a useful real-world application of computer vision.\n",
        "\n",
        "the basic concept was to process video footage, detect humans as anomalies, and trigger an alarm when one appears in the frame.\n",
        "\n",
        "initially, i wrote a batch processing pipeline to extract frames from a video, run detection on each frame, save the annotated frames, and then recombine them into a processed video. i used ffmpeg for splitting the video into images and later for recombining them. here is the core part of the code i used for this approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf64d6be",
      "metadata": {
        "id": "cf64d6be"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import json\n",
        "from ultralytics import YOLO\n",
        "from datetime import timedelta\n",
        "import pygame\n",
        "\n",
        "VIDEO_PATH = \"robbery_clip.mp4\"\n",
        "FRAME_DIR = \"frames2\"\n",
        "OUTPUT_VIDEO = \"output/annotated_video.mp4\"\n",
        "ALARM_FRAMES_DIR = \"output/alarm_frames2\"\n",
        "ANOMALY_LOG = \"output/anomalies.json\"\n",
        "CONFIDENCE_THRESHOLD = 0.4\n",
        "SKIP_FRAMES = 3\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "os.makedirs(FRAME_DIR, exist_ok=True)\n",
        "os.makedirs(ALARM_FRAMES_DIR, exist_ok=True)\n",
        "\n",
        "pygame.init()\n",
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load(\"alarm.mp3\")\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "frame_count = 0\n",
        "anomalies = []\n",
        "\n",
        "print(\"[INFO] Starting anomaly detection...\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "\n",
        "    if frame_count % SKIP_FRAMES != 0:\n",
        "        frame_count += 1\n",
        "        continue\n",
        "\n",
        "    results = model(frame)[0]\n",
        "    detected_person = False\n",
        "\n",
        "    for result in results.boxes:\n",
        "        cls = int(result.cls[0])\n",
        "        conf = float(result.conf[0])\n",
        "        label = model.names[cls]\n",
        "\n",
        "        if conf >= CONFIDENCE_THRESHOLD and label == \"person\":\n",
        "            detected_person = True\n",
        "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    if detected_person:\n",
        "        timestamp = str(timedelta(seconds=frame_count / fps))\n",
        "        anomalies.append({\n",
        "            \"frame\": frame_count,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"object\": \"person\"\n",
        "        })\n",
        "\n",
        "        cv2.imwrite(f\"{ALARM_FRAMES_DIR}/frame_{frame_count}.jpg\", frame)\n",
        "\n",
        "\n",
        "        if not pygame.mixer.music.get_busy():\n",
        "            pygame.mixer.music.play()\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "with open(ANOMALY_LOG, \"w\") as f:\n",
        "    json.dump({\"anomalies\": anomalies}, f, indent=2)\n",
        "\n",
        "print(f\"[DONE] {len(anomalies)} anomalies detected.\")\n",
        "print(f\"Annotated video saved to: {OUTPUT_VIDEO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9795e6",
      "metadata": {
        "id": "9a9795e6"
      },
      "source": [
        "some explanation about what each part of code does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4ea590",
      "metadata": {
        "id": "ce4ea590"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import json\n",
        "from ultralytics import YOLO\n",
        "from datetime import timedelta\n",
        "import pygame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7d3e7d2",
      "metadata": {
        "id": "a7d3e7d2"
      },
      "source": [
        "here, i’m importing all the libraries i need.\n",
        "\n",
        "- `cv2` is for working with videos and images using opencv.\n",
        "- `os` helps with file and folder operations.\n",
        "- `json` is used to save the detected anomalies as a json file.\n",
        "- `YOLO` is the detection model from ultralytics.\n",
        "- `timedelta` helps convert frame number into readable time.\n",
        "- `pygame` is used for playing the alarm sound. i wanted to actually use “playsound from playsound library” but that ended up not working- because it was blocking (waits for sound to finish), it silently failed.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7862782",
      "metadata": {
        "id": "c7862782"
      },
      "outputs": [],
      "source": [
        "VIDEO_PATH = \"robbery_clip.mp4\"\n",
        "FRAME_DIR = \"frames2\"\n",
        "OUTPUT_VIDEO = \"output/annotated_video.mp4\"\n",
        "ALARM_FRAMES_DIR = \"output/alarm_frames2\"\n",
        "ANOMALY_LOG = \"output/anomalies.json\"\n",
        "CONFIDENCE_THRESHOLD = 0.4\n",
        "SKIP_FRAMES = 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e1c1ad",
      "metadata": {
        "id": "75e1c1ad"
      },
      "source": [
        "here i’m setting all the important parameters.\n",
        "\n",
        "- `VIDEO_PATH` is the video file i want to process.\n",
        "- `FRAME_DIR` is where the raw frames will be saved (although in this version i don’t actually use it).\n",
        "- `OUTPUT_VIDEO` is where i’ll save the final annotated video.\n",
        "- `ALARM_FRAMES_DIR` is where i’ll save the frames that contain a person.\n",
        "- `ANOMALY_LOG` is the json file where i log each anomaly.\n",
        "- `CONFIDENCE_THRESHOLD` means i’ll only consider detections if the confidence is more than 0.4.\n",
        "- `SKIP_FRAMES` means i’ll process every 3rd frame to save time.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca49ae06",
      "metadata": {
        "id": "ca49ae06"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"yolov8n.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0599d7",
      "metadata": {
        "id": "2f0599d7"
      },
      "source": [
        "i’m loading the pretrained yolov8n model. this is a lightweight object detection model that already knows how to detect people.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348a7dbc",
      "metadata": {
        "id": "348a7dbc"
      },
      "outputs": [],
      "source": [
        "os.makedirs(FRAME_DIR, exist_ok=True)\n",
        "os.makedirs(ALARM_FRAMES_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265a5739",
      "metadata": {
        "id": "265a5739"
      },
      "source": [
        "i make sure the required folders exist. if they don’t, this creates them so the rest of the code doesn't crash while saving files.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33315826",
      "metadata": {
        "id": "33315826"
      },
      "outputs": [],
      "source": [
        "pygame.init()\n",
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load(\"alarm.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a6f59cb",
      "metadata": {
        "id": "5a6f59cb"
      },
      "source": [
        "i initialize pygame and load an alarm sound. this alarm will play if a person is detected in any frame.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37a5900",
      "metadata": {
        "id": "f37a5900"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (frame_width, frame_height))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d84ebc",
      "metadata": {
        "id": "75d84ebc"
      },
      "source": [
        "here, i open the video using opencv, get the video’s frame rate and dimensions, and set up a video writer object that will write the output with annotations to a new video file.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a457d111",
      "metadata": {
        "id": "a457d111"
      },
      "outputs": [],
      "source": [
        "frame_count = 0\n",
        "anomalies = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ba66bf",
      "metadata": {
        "id": "a7ba66bf"
      },
      "source": [
        "i’m initializing a frame counter and an empty list to store anomaly data (i.e., when a person is detected).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2544c1d1",
      "metadata": {
        "id": "2544c1d1"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] Starting anomaly detection...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35f1a9d",
      "metadata": {
        "id": "b35f1a9d"
      },
      "source": [
        "this is just a debugging message\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fc02fe",
      "metadata": {
        "id": "22fc02fe"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7765988",
      "metadata": {
        "id": "f7765988"
      },
      "source": [
        "i start reading the video frame by frame. if there no more frame, i break out of the loop.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adae622b",
      "metadata": {
        "id": "adae622b"
      },
      "outputs": [],
      "source": [
        "    if frame_count % SKIP_FRAMES != 0:\n",
        "        frame_count += 1\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87460113",
      "metadata": {
        "id": "87460113"
      },
      "source": [
        "i skip every few frames based on the `SKIP_FRAMES` value. this helps speed up the detection process, because i found that analysing too many frames naturally take too long\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11fffb89",
      "metadata": {
        "id": "11fffb89"
      },
      "outputs": [],
      "source": [
        "    results = model(frame)[0]\n",
        "    detected_person = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "856910b0",
      "metadata": {
        "id": "856910b0"
      },
      "source": [
        "i run object detection on the current frame.\n",
        "\n",
        "`results` contains the detection info.\n",
        "\n",
        "i also start with assuming no person is detected yet.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3167a98",
      "metadata": {
        "id": "d3167a98"
      },
      "outputs": [],
      "source": [
        "    for result in results.boxes:\n",
        "        cls = int(result.cls[0])\n",
        "        conf = float(result.conf[0])\n",
        "        label = model.names[cls]\n",
        "\n",
        "        if conf >= CONFIDENCE_THRESHOLD and label == \"person\":\n",
        "            detected_person = True\n",
        "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7fa24ce",
      "metadata": {
        "id": "f7fa24ce"
      },
      "source": [
        "i go through all the boxes detected by yolov8.\n",
        "\n",
        "if the label is “person” and the confidence is above the threshold, i mark that a person is detected.\n",
        "\n",
        "then i draw a red rectangle around the person and add the label text (with confidence score) on the frame.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e93f6b",
      "metadata": {
        "id": "33e93f6b"
      },
      "outputs": [],
      "source": [
        "    if detected_person:\n",
        "        timestamp = str(timedelta(seconds=frame_count / fps))\n",
        "        anomalies.append({\n",
        "            \"frame\": frame_count,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"object\": \"person\"\n",
        "        })\n",
        "\n",
        "        cv2.imwrite(f\"{ALARM_FRAMES_DIR}/frame_{frame_count}.jpg\", frame)\n",
        "\n",
        "        if not pygame.mixer.music.get_busy():\n",
        "            pygame.mixer.music.play()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca21f2f",
      "metadata": {
        "id": "5ca21f2f"
      },
      "source": [
        "if a person was detected in the frame:\n",
        "\n",
        "- i convert the frame number into a time format\n",
        "- i log the detection in the `anomalies` list\n",
        "- i save the frame as an image for reference\n",
        "- if the alarm isn’t already playing, i play it\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba1ee6a",
      "metadata": {
        "id": "2ba1ee6a"
      },
      "outputs": [],
      "source": [
        "    out.write(frame)\n",
        "    frame_count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ef962b",
      "metadata": {
        "id": "a9ef962b"
      },
      "source": [
        "i write the processed frame (with bounding boxes, if any) to the output video.\n",
        "\n",
        "then i increase the frame counter.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b948ad07",
      "metadata": {
        "id": "b948ad07"
      },
      "outputs": [],
      "source": [
        "cap.release()\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c2fd43",
      "metadata": {
        "id": "04c2fd43"
      },
      "source": [
        "i release both the video input and output resources once i’m done processing.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb2f4c9",
      "metadata": {
        "id": "2eb2f4c9"
      },
      "outputs": [],
      "source": [
        "with open(ANOMALY_LOG, \"w\") as f:\n",
        "    json.dump({\"anomalies\": anomalies}, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c82fca",
      "metadata": {
        "id": "74c82fca"
      },
      "source": [
        "i write all the anomalies i collected into a json file, nicely formatted with indentation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cfdffc",
      "metadata": {
        "id": "56cfdffc"
      },
      "outputs": [],
      "source": [
        "print(f\"[DONE] {len(anomalies)} anomalies detected.\")\n",
        "print(f\"Annotated video saved to: {OUTPUT_VIDEO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f5bed3",
      "metadata": {
        "id": "e1f5bed3"
      },
      "source": [
        "i display a final message telling me how many anomalies were found and where the annotated video is saved.\n",
        "\n",
        "---\n",
        "\n",
        "and that’s it — this whole script processes a video, detects people, raises alarms, saves frames with detections, and outputs a full log + video.\n",
        "\n",
        "this code worked well as a prototype, and it included key features such as confidence thresholding, skipping frames to improve speed, drawing bounding boxes, saving anomaly frames, and raising an alarm sound.\n",
        "\n",
        "# burglary detection system\n",
        "\n",
        "however, as i tested it, i realized that batch processing video frames like this—reading from disk, writing to disk, and then combining frames back into a video—was inefficient and not suitable for real-time applications. i needed to detect intrusions as they happen, for example from a live webcam feed or real-time CCTV footage, not after the fact.\n",
        "\n",
        "thus, i moved towards implementing a real-time detection pipeline where frames are captured directly from the camera, processed immediately, and anomalies trigger immediate alarms and visual feedback. this means i would not be able to use ffmpeg anymore- for it was only for breaking “videos” into frames. so,\n",
        "\n",
        "here’s the real-time detection code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95048ba7",
      "metadata": {
        "id": "95048ba7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from datetime import timedelta\n",
        "import pygame\n",
        "\n",
        "CONFIDENCE_THRESHOLD = 0.4\n",
        "SKIP_FRAMES = 3\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "pygame.init()\n",
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load(\"alarm.mp3\")\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % SKIP_FRAMES == 0:\n",
        "        results = model(frame)[0]\n",
        "        detected_person = False\n",
        "\n",
        "        for result in results.boxes:\n",
        "            cls = int(result.cls[0])\n",
        "            conf = float(result.conf[0])\n",
        "            label = model.names[cls]\n",
        "\n",
        "            if conf >= CONFIDENCE_THRESHOLD and label == \"person\":\n",
        "                detected_person = True\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "        if detected_person and not pygame.mixer.music.get_busy():\n",
        "            pygame.mixer.music.play()\n",
        "\n",
        "    cv2.imshow(\"Real-time Anomaly Detection\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5593d752",
      "metadata": {
        "id": "5593d752"
      },
      "source": [
        "detailed explanation of each part:\n",
        "\n",
        "im setting two important constants here.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cbc2ed",
      "metadata": {
        "id": "94cbc2ed"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)  # open default webcam\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30  # default to 30 if 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295c2ca1",
      "metadata": {
        "id": "295c2ca1"
      },
      "source": [
        "im opening the default webcam with `cv2.VideoCapture(0)`.\n",
        "\n",
        "`fps` gets the frames per second from the webcam, and if it returns 0, im just setting it to 30 by default so calculations are stable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde59941",
      "metadata": {
        "id": "cde59941"
      },
      "outputs": [],
      "source": [
        "frame_count = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d63c5ac",
      "metadata": {
        "id": "2d63c5ac"
      },
      "source": [
        "im initializing a counter to keep track of how many frames have been processed.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc04676d",
      "metadata": {
        "id": "fc04676d"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "975132cd",
      "metadata": {
        "id": "975132cd"
      },
      "source": [
        "im starting a loop that keeps running as long as the webcam is feeding frames.\n",
        "\n",
        "if no frame is returned (maybe the webcam was unplugged), then the loop will break.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3356100d",
      "metadata": {
        "id": "3356100d"
      },
      "outputs": [],
      "source": [
        "    if frame_count % SKIP_FRAMES == 0:\n",
        "        results = model(frame)[0]\n",
        "        detected_person = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5532d1c",
      "metadata": {
        "id": "f5532d1c"
      },
      "source": [
        "im processing every 3rd frame (based on `SKIP_FRAMES`).\n",
        "\n",
        "then im passing the frame to the yolov8 model and getting the first result.\n",
        "\n",
        "`detected_person` is a flag that starts as false for each frame.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c44db7",
      "metadata": {
        "id": "83c44db7"
      },
      "outputs": [],
      "source": [
        "        for result in results.boxes:\n",
        "            cls = int(result.cls[0])\n",
        "            conf = float(result.conf[0])\n",
        "            label = model.names[cls]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27776ebf",
      "metadata": {
        "id": "27776ebf"
      },
      "source": [
        "im looping over each detected object in the frame.\n",
        "\n",
        "- `cls` is the class id (like 0 for person).\n",
        "- `conf` is the confidence score for that detection.\n",
        "- `label` gets the actual name of the detected object from the model’s label list.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79a906f",
      "metadata": {
        "id": "a79a906f"
      },
      "outputs": [],
      "source": [
        "            if conf >= CONFIDENCE_THRESHOLD and label == \"person\":\n",
        "                detected_person = True\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13cf2e8e",
      "metadata": {
        "id": "13cf2e8e"
      },
      "source": [
        "if the confidence is high enough and the object is a person, im setting `detected_person` to true.\n",
        "\n",
        "then im drawing a red rectangle around the person and adding the label with the confidence score on top of the box using `cv2.putText`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e743158e",
      "metadata": {
        "id": "e743158e"
      },
      "outputs": [],
      "source": [
        "        if detected_person and not pygame.mixer.music.get_busy():\n",
        "            pygame.mixer.music.play()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2e1347",
      "metadata": {
        "id": "2a2e1347"
      },
      "source": [
        "if a person was detected and the alarm is not already playing, im playing the alarm sound.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4de09d",
      "metadata": {
        "id": "3d4de09d"
      },
      "outputs": [],
      "source": [
        "    cv2.imshow(\"Real-time Anomaly Detection\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b5f493",
      "metadata": {
        "id": "c1b5f493"
      },
      "source": [
        "im showing the video frame with any annotations in a window titled “real-time anomaly detection”.\n",
        "\n",
        "if i press the 'q' key, the loop will break and the program will stop.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7589ce",
      "metadata": {
        "id": "1a7589ce"
      },
      "outputs": [],
      "source": [
        "    frame_count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421b064e",
      "metadata": {
        "id": "421b064e"
      },
      "source": [
        "im increasing the frame count after every loop, so skipping works properly.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6fd159f",
      "metadata": {
        "id": "a6fd159f"
      },
      "outputs": [],
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a33d05",
      "metadata": {
        "id": "74a33d05"
      },
      "source": [
        "once the loop is done, im releasing the webcam and closing all open opencv windows to clean up.\n",
        "\n",
        "---\n",
        "\n",
        "this worked beautifully — it enabled me to open my webcam, detect humans in real-time, draw bounding boxes, and play an alarm sound instantly when a person is detected. this code lets me use my webcam to do real-time detection of people using yolov8, and if someone shows up in the frame, it plays an alarm automatically.\n",
        "\n",
        "# scope for future work\n",
        "\n",
        "next, i considered expanding this system to detect other dangerous objects like knives and guns, which would make the system more robust for security applications.\n",
        "\n",
        "however, i quickly realized that:\n",
        "\n",
        "- the base yolov8n model does not include classes for knives or guns,\n",
        "- reliable detection of such weapons would require custom training on specialized datasets or finding a pre-trained model with these classes,\n",
        "- and training such a model or even searching and integrating one would take more time and effort than i could afford before my exams.\n",
        "\n",
        "thus, i decided to postpone the weapon detection feature for now, focusing on perfecting person detection and real-time alarms first.\n",
        "\n",
        "in summary:\n",
        "\n",
        "- i began with batch video processing using ffmpeg and yolov8, which worked but was inefficient for real-time needs,\n",
        "- i then implemented a real-time webcam anomaly detection pipeline with live bounding box overlays and alarm sounds,\n",
        "- finally, i scoped the possibility of detecting knives and guns but deferred it due to dataset limitations and time constraints.\n",
        "\n",
        "this project taught me a lot about the practical trade-offs between batch and real-time processing, integrating audio alarms, and the realities of deploying computer vision models beyond off-the-shelf datasets.\n",
        "\n",
        "overall, it was an exciting journey from theory to a functioning prototype that can detect burglars and alert users automatically — a small but meaningful application of computer vision technology i’m proud to have built."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}